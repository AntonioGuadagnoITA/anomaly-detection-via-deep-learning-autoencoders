{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "window = 672       # 1 week\n",
    "stride = 8         # 1 hour\n",
    "latent_dim = 10    # Latent dimension\n",
    "epochs = 500       # Number of epochs (no early stopping)\n",
    "batch_size = 8     # Batch size\n",
    "M = 100            # Monte Carlo\n",
    "alpha = 0.5\n",
    "noise_factor = [0.2, 0.2, 0.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparametrization trick\n",
    "def sample_z1(args):\n",
    "    z_mean, z_log_var = args\n",
    "    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1]))\n",
    "    return z_mean + tf.exp(alpha * z_log_var) * eps\n",
    "\n",
    "# Reparametrization trick\n",
    "def sample_z2(args):\n",
    "    z_mean, z_log_var = args\n",
    "    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n",
    "    return z_mean + tf.exp(alpha * z_log_var) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise(tfk.layers.Layer):\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        data, noise_factor = inputs\n",
    "        noise = tf.map_fn(fn=lambda t: tf.random.normal((672,3), 0, 1)*noise_factor, elems=data)\n",
    "        noise_input = data + noise\n",
    "        \n",
    "        return noise_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimScore(tfk.layers.Layer):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq, h_dim = inputs  # seq: batch x x_dim x h_dim*2\n",
    "\n",
    "        S = tf.map_fn(fn=lambda t: tf.linalg.matmul(tf.transpose(t), t), elems=seq) # batch x h_dim x h_dim\n",
    "        S = tf.map_fn(fn=lambda t: t / tf.math.sqrt((tf.cast(h_dim*2, dtype=tf.float32))), elems=S)\n",
    "        A = tf.map_fn(fn=lambda t: tf.keras.activations.softmax(t), elems=S)\n",
    "        C = tf.matmul(seq, A)\n",
    "\n",
    "        return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import Input\n",
    "attention_dim = 10\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "output_shape = X_train.shape[1:]\n",
    "\n",
    "###########\n",
    "# ENCODER #\n",
    "###########\n",
    "\n",
    "encoder_input = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "noisy_input = AddNoise()([encoder_input, noise_factor])\n",
    "\n",
    "h_seq, forward_h, forward_c, backward_h, backward_c = tfkl.Bidirectional(tfkl.LSTM(72, activation='tanh', return_sequences=True, return_state=True))(noisy_input)\n",
    "\n",
    "state_h = tfkl.Concatenate()([forward_h, backward_h])\n",
    "\n",
    "Cdet = SimScore()([h_seq, window])\n",
    "\n",
    "c_mean = tfkl.Dense(attention_dim , activation='linear', name=\"c_mean\")(Cdet)\n",
    "c_log_var = tfkl.Dense(attention_dim , activation='softplus', name=\"c_var\")(Cdet)\n",
    "\n",
    "# Latent representation: mean + log of std.dev.\n",
    "z_mean = tfkl.Dense(latent_dim, activation='linear', name=\"z_mean\")(state_h)\n",
    "z_log_var = tfkl.Dense(latent_dim, activation='softplus', name=\"z_var\")(state_h)\n",
    "\n",
    "# Sampling a vector from the latent distribution\n",
    "z = tfkl.Lambda(sample_z1, name='z')([z_mean, z_log_var])\n",
    "c = tfkl.Lambda(sample_z2, name='c')([c_mean, c_log_var])\n",
    "\n",
    "encoder = tfk.Model(encoder_input, [z_mean, z_log_var, z, c_mean, c_log_var, c], name='encoder')\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# DECODER #\n",
    "###########\n",
    "\n",
    "z_inputs = Input(shape=(latent_dim, ), name='decoder_input_1')\n",
    "c_inputs = Input(shape=(window, attention_dim), name='decoder_input_2')\n",
    "\n",
    "repeated = tfkl.RepeatVector(window)(z_inputs)\n",
    "concat = tfkl.Concatenate(axis=-1)([repeated, c_inputs])\n",
    "\n",
    "x = tfkl.Bidirectional(tfkl.LSTM(72, return_sequences=True))(concat)\n",
    "\n",
    "# OPZIONE 1 - Dense Layers come nel paper\n",
    "mu = tfkl.Dense(3, activation='linear', name=\"mu\")(x)\n",
    "log_sigma = tfkl.Dense(3, activation='softplus', name=\"sigma\")(x)\n",
    "\n",
    "# Define and summarize decoder model\n",
    "decoder = tfk.Model([z_inputs, c_inputs], [mu, log_sigma], name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tfk.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n",
    "        self.likelihood_tracker = tfk.metrics.Mean(name=\"likelihood\")\n",
    "        self.kl_loss_z_tracker = tfk.metrics.Mean(name=\"kl_loss_z\")\n",
    "        self.kl_loss_c_tracker = tfk.metrics.Mean(name=\"kl_loss_c\")\n",
    "        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.likelihood_tracker,\n",
    "            self.kl_loss_z_tracker,\n",
    "            self.kl_loss_c_tracker,\n",
    "            self.reconstruction_loss_tracker\n",
    "        ]\n",
    "    \n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            sample = None\n",
    "            encoder_mu, encoder_log_var, z, c_mean, c_log_var, c = self.encoder(data)\n",
    "            decoder_mu, decoder_log_sigma = self.decoder([z,c])\n",
    "            decoder_sigma = tf.exp(decoder_log_sigma)\n",
    "            \n",
    "            pdf_laplace = tfp.distributions.Laplace(decoder_mu, decoder_sigma, validate_args=True, name='Laplace')\n",
    "                        \n",
    "            for _ in range(M):\n",
    "                sample = pdf_laplace.sample() if sample is None else sample + pdf_laplace.sample()\n",
    "                             \n",
    "            likelihood = -(pdf_laplace.log_prob(data))\n",
    "            likelihood = tf.reduce_mean(likelihood, axis=-1)\n",
    "            likelihood = tf.reduce_mean(likelihood, axis=-1)\n",
    "                \n",
    "            decoder_output = sample/M\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n",
    "            \n",
    "            kl_loss_z = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n",
    "            kl_loss_z = tf.reduce_mean(tf.reduce_sum(kl_loss_z, axis=1))\n",
    "            \n",
    "            kl_loss_c = -0.5 * (1 + c_log_var - tf.square(c_mean) - tf.exp(c_log_var))\n",
    "            kl_loss_c = tf.keras.backend.sum(kl_loss_c, axis=1)\n",
    "            kl_loss_c = tf.reduce_mean(kl_loss_c, axis=1)\n",
    "\n",
    "            total_loss = 4*likelihood + 2*(kl_loss_z + 6*kl_loss_c) + reconstruction_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.likelihood_tracker.update_state(likelihood)\n",
    "        self.kl_loss_z_tracker.update_state(kl_loss_z)\n",
    "        self.kl_loss_c_tracker.update_state(kl_loss_c)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"likelihood\": self.likelihood_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result()\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    def test_step(self, data): # https://github.com/keras-team/keras-io/issues/38\n",
    "\n",
    "        sample = None\n",
    "        encoder_mu, encoder_log_var, z, c_mean, c_log_var, c = self.encoder(data)\n",
    "        decoder_mu, decoder_log_sigma = self.decoder([z,c])\n",
    "        decoder_sigma = tf.exp(decoder_log_sigma)\n",
    "            \n",
    "        pdf_laplace = tfp.distributions.Laplace(decoder_mu, decoder_sigma, validate_args=True, name='Laplace')\n",
    "                        \n",
    "        for _ in range(M):\n",
    "            sample = pdf_laplace.sample() if sample is None else sample + pdf_laplace.sample()\n",
    "                             \n",
    "        likelihood = -(pdf_laplace.log_prob(data))\n",
    "        likelihood = tf.reduce_mean(likelihood, axis=-1)\n",
    "        likelihood = tf.reduce_mean(likelihood, axis=-1)\n",
    "                \n",
    "        decoder_output = sample/M\n",
    "        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n",
    "            \n",
    "        kl_loss_z = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n",
    "        kl_loss_z = tf.reduce_mean(tf.reduce_sum(kl_loss_z, axis=1))\n",
    "            \n",
    "        kl_loss_c = -0.5 * (1 + c_log_var - tf.square(c_mean) - tf.exp(c_log_var))\n",
    "        kl_loss_c = tf.keras.backend.sum(kl_loss_c, axis=1)\n",
    "        kl_loss_c = tf.reduce_mean(kl_loss_c, axis=1)\n",
    "\n",
    "        total_loss = 4*likelihood + 2*(kl_loss_z + 6*kl_loss_c) + reconstruction_loss\n",
    "            \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.likelihood_tracker.update_state(likelihood)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_z_tracker.update_state(kl_loss_z)\n",
    "        self.kl_loss_c_tracker.update_state(kl_loss_c)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"likelihood\": self.likelihood_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_c\": self.kl_loss_c_tracker.result(),\n",
    "            \"kl_z\": self.kl_loss_z_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "vae.compile(optimizer=tfk.optimizers.Adam())\n",
    "\n",
    "vae.fit(x = X_train,\n",
    "        validation_data = (X_val, None),\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        callbacks=[tfk.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True), tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, min_lr=1e-5)]\n",
    "       )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
