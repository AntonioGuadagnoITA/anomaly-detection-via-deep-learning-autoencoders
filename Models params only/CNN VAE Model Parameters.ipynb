{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":1867.331586,"end_time":"2022-08-06T17:50:26.749551","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-08-06T17:19:19.417965","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# Training parameters\n\nwindow = 672       # 1 week\nstride = 4         # 1 hour\nlatent_dim = 10    # Autoencoder latent dimension\nepochs = 150       # Number of epochs\nbatch_size = 8     # Batch size\nM = 200            # Montecarlo\nf = 5              # Filters' dimensions (conv layers)","metadata":{"_cell_guid":"940d6c9b-eeb2-4b88-84e0-0802a2bb8912","_uuid":"daee88fd-c068-4b12-aa80-f5dff1c7b11b","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.029765,"end_time":"2022-08-06T17:19:40.557329","exception":false,"start_time":"2022-08-06T17:19:40.527564","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\n\nfrom keras import backend as K\nfrom tensorflow.keras import Input\n\ninput_shape = X_train.shape[1:]\noutput_shape = X_train.shape[1:]\n\n###########\n# ENCODER #\n###########\n\nencoder_input = tf.keras.Input(shape=input_shape)\n\nx = tfkl.Conv1D(16, f, activation=\"relu\", strides=1, padding=\"same\")(encoder_input)\nx = tfkl.MaxPool1D(pool_size=2, strides=2)(x)\nx = tfkl.Conv1D(32, f, activation=\"relu\", strides=1, padding=\"same\")(x)\nx = tfkl.MaxPool1D(pool_size=2, strides=2)(x)\nx = tfkl.Conv1D(64, f, activation=\"relu\", strides=1, padding=\"same\")(x)\nx = tfkl.MaxPool1D(pool_size=2, strides=2)(x)\n\nx = tfkl.Flatten()(x)\nx = tfkl.Dense(latent_dim, activation='linear')(x)\n\n# Latent representation: mean + log of std.dev.\nz_mu = tfkl.Dense(latent_dim, name='latent_mu')(x) # Mean\nz_sigma = tfkl.Dense(latent_dim, name='latent_sigma')(x) # Std.Dev.\n\n# Reparametrization trick\ndef sample_z1(args):\n    z_mean, z_log_var = args\n    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1]))\n    return z_mean + tf.exp(alpha * z_log_var) * eps\n    \n\n# Sampling a vector from the latent distribution\nz = tfkl.Lambda(sample_z1, output_shape=(latent_dim, ), name='z')([z_mu, z_sigma])\n\nencoder = tfk.Model(encoder_input, [z_mu, z_sigma, z], name='encoder')\nprint(encoder.summary())","metadata":{"_cell_guid":"2d9f076e-ae1a-4720-b536-fca5412f83f4","_uuid":"8a40c700-5345-4ee4-95fa-07f1e57173eb","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.232773,"end_time":"2022-08-06T17:19:41.219267","exception":false,"start_time":"2022-08-06T17:19:40.986494","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########\n# DECODER #\n###########\n\ndecoder_input = Input(shape=(latent_dim, ), name='decoder_input')\nx = tfkl.Dense(units=42*X_train.shape[2])(decoder_input)\nx = tfkl.Reshape((42,X_train.shape[2]))(x)\nx = tfkl.Conv1DTranspose(64,f,2, padding='same', activation='relu')(x)\nx = tfkl.Conv1DTranspose(32,f,2, padding='same', activation='relu')(x)\nx = tfkl.Conv1DTranspose(16,f,2, padding='same', activation='relu')(x)\ndecoder_output = tfkl.Conv1DTranspose(X_train.shape[2],f,2, padding='same', activation='linear')(x)\n\n# Define and summarize decoder model\ndecoder = tfk.Model(decoder_input, decoder_output, name='decoder')\ndecoder.summary()","metadata":{"_cell_guid":"6c1d6b54-49d6-48f1-a6f3-38175181fc5f","_uuid":"840b7a8f-5a52-439d-9686-79891debd8d1","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.14103,"end_time":"2022-08-06T17:19:41.382918","exception":false,"start_time":"2022-08-06T17:19:41.241888","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VAE(tfk.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = tfk.metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def train_step(self, y_true):\n        with tf.GradientTape() as tape:\n            \n            encoder_mu, encoder_log_var, z = self.encoder(y_true)\n            y_predict = self.decoder(z)\n          \n            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(y_true, y_predict), axis=1))\n            \n            kl_loss = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n            \n            total_loss = reconstruction_loss + kl_loss\n            \n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        \n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        \n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result()\n        }\n    \n    def test_step(self, data): #https://github.com/keras-team/keras-io/issues/38\n\n        encoder_mu, encoder_log_var, z = self.encoder(data)\n        y_predict = self.decoder(z)\n          \n        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, y_predict), axis=1))\n            \n        kl_loss = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n            \n        total_loss = reconstruction_loss + kl_loss\n        \n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss\n        }","metadata":{"_cell_guid":"6abeeb1d-b047-4fe9-a3e5-ed5bb59bf70d","_uuid":"36931ab9-f55f-428f-8cad-6d52e9521aec","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.04068,"end_time":"2022-08-06T17:19:41.444632","exception":false,"start_time":"2022-08-06T17:19:41.403952","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae = VAE(encoder, decoder)\nvae.compile(optimizer=tfk.optimizers.Adam())\nvae.fit(x = X_train,\n        validation_data = (X_val, None),\n        epochs=epochs, \n        batch_size=batch_size)\nvae.fit(x = X_train,\n        validation_data = (X_val, None),\n        epochs=epochs, \n        batch_size=batch_size,\n        callbacks=[tfk.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)])","metadata":{"_cell_guid":"3a2dfb11-ea50-4bf2-9c89-4ac6e312f9e4","_uuid":"893a7da0-7591-4c34-b59e-96e748d40a55","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":1695.057488,"end_time":"2022-08-06T17:47:56.523065","exception":false,"start_time":"2022-08-06T17:19:41.465577","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}