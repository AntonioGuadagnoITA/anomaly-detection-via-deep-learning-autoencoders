{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":10469.392834,"end_time":"2022-08-27T22:55:21.106462","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-08-27T20:00:51.713628","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# Training parameters\n\nwindow = 672       # 1 week\nstride = 4         # 1 hour\nlatent_dim = 10    # Autoencoder latent dimension\nepochs = 150       # Number of epochs\nbatch_size = 8     # Batch size\nM = 200            # Montecarlo\nf = 5              # Filters' dimensions (conv layers)","metadata":{"_cell_guid":"940d6c9b-eeb2-4b88-84e0-0802a2bb8912","_uuid":"daee88fd-c068-4b12-aa80-f5dff1c7b11b","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.02836,"end_time":"2022-08-27T20:01:14.936092","exception":false,"start_time":"2022-08-27T20:01:14.907732","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\n\nfrom keras import backend as K\nfrom tensorflow.keras import Input\n\ninput_shape = X_train.shape[1:]\noutput_shape = X_train.shape[1:]\n\n###########\n# ENCODER #\n###########\n\nencoder_input = tf.keras.Input(shape=input_shape)\n\nx = tfkl.Conv1D(16, f, activation=\"relu\", strides=1, padding=\"same\")(encoder_input)\nx = tfkl.MaxPool1D(pool_size=2, strides=2)(x)\nx = tfkl.Conv1D(32, f, activation=\"relu\", strides=1, padding=\"same\")(x)\nx = tfkl.MaxPool1D(pool_size=2, strides=2)(x)\nx = tfkl.Conv1D(64, f, activation=\"relu\", strides=1, padding=\"same\")(x)\nx = tfkl.MaxPool1D(pool_size=2, strides=2)(x)\n\nx = tfkl.Flatten()(x)\nx = tfkl.Dense(latent_dim, activation='linear')(x)\n\n# Latent representation: mean + log of std.dev.\nz_mu = tfkl.Dense(latent_dim, name='latent_mu')(x) # Mean\nz_log_var = tfkl.Dense(latent_dim, name='latent_log_var')(x) # Log Var\n\n# Reparametrization trick\ndef sample_z1(args):\n    z_mean, z_log_var = args\n    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1]))\n    return z_mean + tf.exp(alpha * z_log_var) * eps\n    \n\n# Sampling a vector from the latent distribution\nz = tfkl.Lambda(sample_z1, output_shape=(latent_dim, ), name='z')([z_mu, z_log_var])\n\nencoder = tfk.Model(encoder_input, [z_mu, z_log_var, z], name='encoder')\nprint(encoder.summary())","metadata":{"_cell_guid":"2d9f076e-ae1a-4720-b536-fca5412f83f4","_uuid":"8a40c700-5345-4ee4-95fa-07f1e57173eb","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.471241,"end_time":"2022-08-27T20:01:15.901698","exception":false,"start_time":"2022-08-27T20:01:15.430457","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###########\n# DECODER #\n###########\n\ndecoder_input = Input(shape=(latent_dim, ), name='decoder_input')\nx = tfkl.Dense(units=42*X_train.shape[2])(decoder_input)\nx = tfkl.Reshape((42,X_train.shape[2]))(x)\nx = tfkl.Conv1DTranspose(64,f,2, padding='same', activation='relu')(x)\nx = tfkl.Conv1DTranspose(32,f,2, padding='same', activation='relu')(x)\nx = tfkl.Conv1DTranspose(16,f,2, padding='same', activation='relu')(x)\nx = tfkl.Conv1DTranspose(X_train.shape[2],f,2, padding='same', activation='linear')(x)\n\nmu = tfkl.Conv1D(X_train.shape[2],2,1, padding='same', name='mu')(x)\nlog_var = tfkl.Conv1D(X_train.shape[2],2,1, padding='same', name='log_var')(x)\n\n# RECONSTRUCTION FOR THE PLOT (NO TRAINING)\n\n# Reparametrization trick\ndef sample_z2(args):\n    z_mean, z_log_var = args\n    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n    return z_mean + tf.exp(alpha * z_log_var) * eps\n\ndecoder_output = tfkl.Lambda(sample_z2, name='decoder_output')([mu, log_var])\n\n# Define and summarize decoder model\ndecoder = tfk.Model(decoder_input, [mu, log_var, decoder_output], name='decoder')\n\ndecoder.summary()","metadata":{"_cell_guid":"6c1d6b54-49d6-48f1-a6f3-38175181fc5f","_uuid":"840b7a8f-5a52-439d-9686-79891debd8d1","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.185457,"end_time":"2022-08-27T20:01:16.106468","exception":false,"start_time":"2022-08-27T20:01:15.921011","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VAE(tfk.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n        self.likelihood_tracker = tfk.metrics.Mean(name=\"likelihood\")\n        self.kl_loss_tracker = tfk.metrics.Mean(name=\"kl_loss\")\n        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.likelihood_tracker,\n            self.kl_loss_tracker,\n            self.reconstruction_loss_tracker\n        ]\n    \n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            \n            # Reparametrization trick\n            def sample_z2(args):\n                z_mean, z_log_var = args\n                eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n                return z_mean + tf.exp(alpha * z_log_var) * eps\n            \n            encoder_mu, encoder_log_var, z = self.encoder(data)\n            decoder_mu, decoder_log_var, _ = self.decoder(z)\n            decoder_sigma = tf.exp(alpha * decoder_log_var)\n                             \n            pdf_normal = tfp.distributions.MultivariateNormalDiag(decoder_mu, decoder_sigma, validate_args=True, name='Gauss')\n            likelihood = -(pdf_normal.log_prob(data))\n            likelihood = tf.reduce_mean(likelihood, axis=-1)\n            likelihood = tf.reduce_mean(likelihood, axis=-1)\n                \n            decoder_output = tfkl.Lambda(sample_z2, output_shape=input_shape, name='decoder_output')([decoder_mu, decoder_log_var])\n            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n            \n            kl_loss = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n\n            total_loss = likelihood + kl_loss + reconstruction_loss\n            \n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.likelihood_tracker.update_state(likelihood)\n        self.kl_loss_tracker.update_state(kl_loss)\n        \n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"likelihood\": self.likelihood_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result()\n        }\n    \n    \n    \n    def test_step(self, data): # https://github.com/keras-team/keras-io/issues/38\n\n        # Reparametrization trick\n        def sample_z2(args):\n            z_mean, z_log_var = args\n            eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n            return z_mean + tf.exp(alpha * z_log_var) * eps\n            \n        encoder_mu, encoder_log_var, z = self.encoder(data)\n        decoder_mu, decoder_log_var, _ = self.decoder(z)\n        decoder_sigma = tf.exp(alpha * decoder_log_var)\n                             \n        pdf_normal = tfp.distributions.MultivariateNormalDiag(decoder_mu, decoder_sigma, validate_args=True, name='Gauss')\n        likelihood = -(pdf_normal.log_prob(data))\n        likelihood = tf.reduce_mean(likelihood, axis=-1)\n        likelihood = tf.reduce_mean(likelihood, axis=-1)\n                \n        decoder_output = tfkl.Lambda(sample_z2, output_shape=input_shape, name='decoder_output')([decoder_mu, decoder_log_var])\n        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n            \n        kl_loss = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n\n        total_loss = likelihood + kl_loss + reconstruction_loss\n            \n        self.total_loss_tracker.update_state(total_loss)\n        self.likelihood_tracker.update_state(likelihood)\n        self.kl_loss_tracker.update_state(kl_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        \n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"likelihood\": self.likelihood_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result()\n        }","metadata":{"_cell_guid":"6abeeb1d-b047-4fe9-a3e5-ed5bb59bf70d","_uuid":"36931ab9-f55f-428f-8cad-6d52e9521aec","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.049617,"end_time":"2022-08-27T20:01:16.175199","exception":false,"start_time":"2022-08-27T20:01:16.125582","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae = VAE(encoder, decoder)\nvae.compile(optimizer=tfk.optimizers.Adam())\nvae.fit(x = X_train,\n        validation_data = (X_val, None),\n        epochs=epochs, \n        batch_size=batch_size)\nvae.fit(x = X_train,\n        validation_data = (X_val, None),\n        epochs=epochs, \n        batch_size=batch_size,\n        callbacks=[tfk.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)])","metadata":{"_cell_guid":"3a2dfb11-ea50-4bf2-9c89-4ac6e312f9e4","_uuid":"893a7da0-7591-4c34-b59e-96e748d40a55","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":2624.045934,"end_time":"2022-08-27T20:45:00.240137","exception":false,"start_time":"2022-08-27T20:01:16.194203","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}