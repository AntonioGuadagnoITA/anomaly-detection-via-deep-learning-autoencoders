{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 14348.48869,
      "end_time": "2022-10-26T14:03:33.948105",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-10-26T10:04:25.459415",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "\n",
        "window = 672       # 1 week\n",
        "stride = 4         # 1 hour\n",
        "latent_dim = 10    # Autoencoder latent dimension\n",
        "epochs = 150       # Number of epochs\n",
        "batch_size = 8     # Batch size\n",
        "M = 200            # Montecarlo"
      ],
      "metadata": {
        "_cell_guid": "ee9dc1b8-8938-45b1-a268-fd5dbcfd54dc",
        "_uuid": "6eec9bc4-88cc-4d5a-b660-23b78027559a",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": 0.022987,
          "end_time": "2022-10-26T10:04:42.510591",
          "exception": false,
          "start_time": "2022-10-26T10:04:42.487604",
          "status": "completed"
        },
        "tags": [],
        "id": "KYeq4fkZo1-f"
      },
      "execution_count": null,
      "outputs": [],
      "id": "KYeq4fkZo1-f"
    },
    {
      "cell_type": "code",
      "source": [
        "class SimScore(tfk.layers.Layer):\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq, h_dim = inputs  # seq: batch x x_dim x h_dim*2\n",
        "\n",
        "        S = tf.map_fn(fn=lambda t: tf.linalg.matmul(tf.transpose(t), t), elems=seq) # batch x h_dim x h_dim\n",
        "        S = tf.map_fn(fn=lambda t: t / tf.math.sqrt((tf.cast(h_dim*2, dtype=tf.float32))), elems=S)\n",
        "        A = tf.map_fn(fn=lambda t: tf.keras.activations.softmax(t), elems=S)\n",
        "        C = tf.matmul(seq, A)\n",
        "\n",
        "        return C"
      ],
      "metadata": {
        "_cell_guid": "c7fc4192-cc4a-4c0f-9224-442633b12811",
        "_uuid": "0ff6e0e9-3718-4233-8be7-1a0e061fc0f8",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": 0.025498,
          "end_time": "2022-10-26T10:04:43.200090",
          "exception": false,
          "start_time": "2022-10-26T10:04:43.174592",
          "status": "completed"
        },
        "tags": [],
        "id": "sQ7GdGAzo1-h"
      },
      "execution_count": null,
      "outputs": [],
      "id": "sQ7GdGAzo1-h"
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNoise(tfk.layers.Layer):\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        data, noise_factor = inputs\n",
        "        noise = tf.map_fn(fn=lambda t: tf.random.normal((672,2), 0, 1)*noise_factor, elems=data)\n",
        "        noise_input = data + noise\n",
        "        \n",
        "        return noise_input"
      ],
      "metadata": {
        "_cell_guid": "98b731e6-2167-448e-b7db-329afdee130e",
        "_uuid": "9cf19f28-650b-4f3f-97e6-f1f5fd03a1c7",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": 0.023645,
          "end_time": "2022-10-26T10:04:43.239491",
          "exception": false,
          "start_time": "2022-10-26T10:04:43.215846",
          "status": "completed"
        },
        "tags": [],
        "id": "dvyacNGWo1-i"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dvyacNGWo1-i"
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the model\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "output_shape = X_train.shape[1:]\n",
        "\n",
        "from keras import backend as K\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "###########\n",
        "# ENCODER #\n",
        "###########\n",
        "\n",
        "encoder_input = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "h_seq, forward_h, forward_c = tfkl.LSTM(64, return_sequences=True, return_state=True)(encoder_input)\n",
        "\n",
        "Cdet = SimScore()([h_seq, window])\n",
        "\n",
        "c_mean = tfkl.Dense(window, activation='linear', name=\"c_mean\")(Cdet)\n",
        "c_log_var = tfkl.Dense(window, activation='softplus', name=\"c_var\")(Cdet)\n",
        "\n",
        "# Latent representation: mean + log of std.dev.\n",
        "z_mean = tfkl.Dense(latent_dim, activation='linear', name=\"z_mean\")(forward_h)\n",
        "z_log_var = tfkl.Dense(latent_dim, activation='softplus', name=\"z_var\")(forward_h)\n",
        "\n",
        "# Reparametrization trick\n",
        "def sample_z1(args):\n",
        "    z_mean, z_log_var = args\n",
        "    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1]))\n",
        "    return z_mean + tf.exp(alpha * z_log_var) * eps\n",
        "\n",
        "# Reparametrization trick\n",
        "def sample_z2(args):\n",
        "    z_mean, z_log_var = args\n",
        "    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n",
        "    return z_mean + tf.exp(alpha * z_log_var) * eps\n",
        "    \n",
        "# Sampling a vector from the latent distribution\n",
        "z = tfkl.Lambda(sample_z1, name='z')([z_mean, z_log_var])\n",
        "c = tfkl.Lambda(sample_z2, name='c')([c_mean, c_log_var])\n",
        "\n",
        "encoder = tfk.Model(encoder_input, [z_mean, z_log_var, z, c_mean, c_log_var, c], name='encoder')\n",
        "print(encoder.summary())"
      ],
      "metadata": {
        "papermill": {
          "duration": 3.287207,
          "end_time": "2022-10-26T10:04:46.542330",
          "exception": false,
          "start_time": "2022-10-26T10:04:43.255123",
          "status": "completed"
        },
        "tags": [],
        "id": "OdueRAK3o1-j"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OdueRAK3o1-j"
    },
    {
      "cell_type": "code",
      "source": [
        "###########\n",
        "# DECODER #\n",
        "###########\n",
        "\n",
        "z_inputs = Input(shape=(latent_dim, ), name='decoder_input_1')\n",
        "c_inputs = Input(shape=(window, window), name='decoder_input_2')\n",
        "\n",
        "repeated = tf.transpose(tfkl.RepeatVector(window)(z_inputs), perm = [0, 2, 1])\n",
        "concat = tf.transpose( tfkl.Concatenate(axis=1)([repeated, c_inputs]), perm = [0, 2, 1] )\n",
        "\n",
        "x = tfkl.LSTM(64, return_sequences=True)(concat)\n",
        "x = tfkl.TimeDistributed(tfkl.Dense(output_shape[1]))(x)\n",
        "\n",
        "mu = tfkl.Conv1D(X_train.shape[2],2,1, padding='same', name='mu')(x)\n",
        "sigma = tfkl.Conv1D(X_train.shape[2],2,1, padding='same', name='sigma')(x)\n",
        "\n",
        "\n",
        "# REPRESENTATION USED ONLY FOR GRAPHICAL PURPOSES\n",
        "\n",
        "# Reparametrization trick\n",
        "def sample_z2(args):\n",
        "    z_mean, z_log_var = args\n",
        "    eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n",
        "    return z_mean + tf.exp(alpha * z_log_var) * eps\n",
        "\n",
        "decoder_output = tfkl.Lambda(sample_z2, output_shape=(672, 2), name='decoder_output')([mu, sigma])\n",
        "\n",
        "# Define and summarize decoder model\n",
        "decoder = tfk.Model([z_inputs, c_inputs], [mu, sigma, decoder_output], name='decoder')\n",
        "decoder.summary()"
      ],
      "metadata": {
        "_cell_guid": "d26e93e1-6019-473d-a3e4-757bab505a46",
        "_uuid": "b11ee25e-c328-4870-ac28-8b14a85b217f",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": 0.277946,
          "end_time": "2022-10-26T10:04:46.836621",
          "exception": false,
          "start_time": "2022-10-26T10:04:46.558675",
          "status": "completed"
        },
        "tags": [],
        "id": "GdBbm3Q4o1-k"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GdBbm3Q4o1-k"
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(tfk.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n",
        "        self.likelihood_tracker = tfk.metrics.Mean(name=\"likelihood\")\n",
        "        self.kl_loss_z_tracker = tfk.metrics.Mean(name=\"kl_loss_z\")\n",
        "        self.kl_loss_c_tracker = tfk.metrics.Mean(name=\"kl_loss_c\")\n",
        "        self.reconstruction_loss_tracker = tfk.metrics.Mean(name=\"reconstruction_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.likelihood_tracker,\n",
        "            self.kl_loss_z_tracker,\n",
        "            self.kl_loss_c_tracker,\n",
        "            self.reconstruction_loss_tracker\n",
        "        ]\n",
        "    \n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            \n",
        "            # Reparametrization trick\n",
        "            def sample_z2(args):\n",
        "                z_mean, z_log_var = args\n",
        "                eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n",
        "                return z_mean + tf.exp(alpha * z_log_var) * eps\n",
        "            \n",
        "            encoder_mu, encoder_log_var, z, c_mean, c_log_var, c = self.encoder(data)\n",
        "            decoder_mu, decoder_log_var, _ = self.decoder([z,c])\n",
        "            decoder_sigma = tf.exp(decoder_log_var)\n",
        "                             \n",
        "            pdf_normal = tfp.distributions.MultivariateNormalDiag(decoder_mu, decoder_sigma, validate_args=True, name='Gauss')\n",
        "            likelihood = -(pdf_normal.log_prob(data))\n",
        "            likelihood = tf.reduce_mean(likelihood, axis=-1)\n",
        "            likelihood = tf.reduce_mean(likelihood, axis=-1)\n",
        "                \n",
        "            decoder_output = tfkl.Lambda(sample_z2, output_shape=input_shape, name='decoder_output')([decoder_mu, decoder_log_var])\n",
        "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n",
        "            \n",
        "            kl_loss_z = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n",
        "            kl_loss_z = tf.reduce_mean(tf.reduce_sum(kl_loss_z, axis=1))\n",
        "            \n",
        "            kl_loss_c = -0.5 * (1 + c_log_var - tf.square(c_mean) - tf.exp(c_log_var))\n",
        "            kl_loss_c = tf.keras.backend.sum(kl_loss_c, axis=1)\n",
        "            kl_loss_c = tf.reduce_mean(kl_loss_c, axis=1)\n",
        "\n",
        "            total_loss = 3*likelihood + kl_loss_z + 0.5*kl_loss_c + reconstruction_loss\n",
        "            \n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.likelihood_tracker.update_state(likelihood)\n",
        "        self.kl_loss_z_tracker.update_state(kl_loss_z)\n",
        "        self.kl_loss_c_tracker.update_state(kl_loss_c)\n",
        "        \n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"likelihood\": self.likelihood_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result()\n",
        "        }\n",
        "    \n",
        "    \n",
        "    \n",
        "    def test_step(self, data): # https://github.com/keras-team/keras-io/issues/38\n",
        "\n",
        "        # Reparametrization trick\n",
        "        def sample_z2(args):\n",
        "            z_mean, z_log_var = args\n",
        "            eps = tf.keras.backend.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1], K.int_shape(z_mean)[2]))\n",
        "            return z_mean + tf.exp(alpha * z_log_var) * eps\n",
        "            \n",
        "        encoder_mu, encoder_log_var, z, c_mean, c_log_var, c = self.encoder(data)\n",
        "        decoder_mu, decoder_log_var, _ = self.decoder([z,c])\n",
        "        decoder_sigma = tf.exp(decoder_log_var)\n",
        "                             \n",
        "        pdf_normal = tfp.distributions.MultivariateNormalDiag(decoder_mu, decoder_sigma, validate_args=True, name='Gauss')\n",
        "        likelihood = -(pdf_normal.log_prob(data))\n",
        "        likelihood = tf.reduce_mean(likelihood, axis=-1)\n",
        "        likelihood = tf.reduce_mean(likelihood, axis=-1)\n",
        "                \n",
        "        decoder_output = tfkl.Lambda(sample_z2, output_shape=input_shape, name='decoder_output')([decoder_mu, decoder_log_var])\n",
        "        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tfk.losses.mse(data, decoder_output), axis=1))\n",
        "            \n",
        "        kl_loss_z = -0.5 * (1 + encoder_log_var - tf.square(encoder_mu) - tf.exp(encoder_log_var))\n",
        "        kl_loss_z = tf.reduce_mean(tf.reduce_sum(kl_loss_z, axis=1))\n",
        "            \n",
        "        kl_loss_c = -0.5 * (1 + c_log_var - tf.square(c_mean) - tf.exp(c_log_var))\n",
        "        kl_loss_c = tf.keras.backend.sum(kl_loss_c, axis=1)\n",
        "        kl_loss_c = tf.reduce_mean(kl_loss_c, axis=1)\n",
        "\n",
        "        total_loss = 3*likelihood + kl_loss_z + 0.5*kl_loss_c + reconstruction_loss\n",
        "            \n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.likelihood_tracker.update_state(likelihood)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_z_tracker.update_state(kl_loss_z)\n",
        "        self.kl_loss_c_tracker.update_state(kl_loss_c)\n",
        "        \n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"likelihood\": self.likelihood_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result()\n",
        "        }"
      ],
      "metadata": {
        "_cell_guid": "d57c2bff-ae5a-4c55-90b8-607a3c3f7256",
        "_uuid": "6dad6fcf-5973-4f2a-9cde-dceec79b78b5",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": 0.042608,
          "end_time": "2022-10-26T10:04:46.895595",
          "exception": false,
          "start_time": "2022-10-26T10:04:46.852987",
          "status": "completed"
        },
        "tags": [],
        "id": "RPkuKIlNo1-l"
      },
      "execution_count": null,
      "outputs": [],
      "id": "RPkuKIlNo1-l"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=tfk.optimizers.Adam())\n",
        "vae.fit(x = X_train,\n",
        "        validation_data = (X_val, None),\n",
        "        epochs=epochs, \n",
        "        batch_size=batch_size,\n",
        "        callbacks=[\n",
        "            tfk.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]\n",
        "       )"
      ],
      "metadata": {
        "_cell_guid": "18773f3d-9d9c-42d2-8144-63471bbddd43",
        "_uuid": "e4065470-c134-43d7-924b-44d32fb767dd",
        "jupyter": {
          "outputs_hidden": false
        },
        "papermill": {
          "duration": 12550.713095,
          "end_time": "2022-10-26T13:33:57.625050",
          "exception": false,
          "start_time": "2022-10-26T10:04:46.911955",
          "status": "completed"
        },
        "tags": [],
        "id": "IzSElSA6o1-n"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IzSElSA6o1-n"
    }
  ]
}